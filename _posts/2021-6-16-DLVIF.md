---
title: "Learning-base Autofocus Metric for Rigid Motion Compensation"
permalink: "/researchPosts/DLVIF/"
layout: post
categories: media
---

# Using Neural Network as Autofocus Metric in Rigid Motion Compensation

## [2022] Reference-free learning-based similarity metric for motion compensation in cone-beam CT

[H. Huang, J.H. Siewerdsen, W. Zbijewski, C. R. Weiss, M. Unberath, T. Ehtiati, A. Sisniega, “Reference-free learning-based similarity metric for motion compensation in cone-beam CT,” in Physics in Medicine & Biology. ](https://doi.org/10.1088/1361-6560/ac749a)

*Purpose*. Patient motion artifacts present a prevalent challenge to image quality in interventional cone-beam CT (CBCT). We propose a novel reference-free similarity metric (DL-VIF) that leverages the capability of deep convolutional neural networks (CNN) to learn features associated with motion artifacts within realistic anatomical features. DL-VIF aims to address shortcomings of conventional metrics of motion-induced image quality degradation that favor characteristics associated with motion-free images, such as sharpness or piecewise constancy, but lack any awareness of the underlying anatomy, potentially promoting images depicting unrealistic image content. DL-VIF was integrated in an autofocus motion compensation framework to test its performance for motion estimation in interventional CBCT.

*Methods*. DL-VIF is a reference-free surrogate for the previously reported visual image fidelity (VIF) metric, computed against a motion-free reference, generated using a CNN trained using simulated motion-corrupted and motion-free CBCT data. Relatively shallow (2-ResBlock) and deep (3-Resblock) CNN architectures were trained and tested to assess sensitivity to motion artifacts and generalizability to unseen anatomy and motion patterns. DL-VIF was integrated into an autofocus framework for rigid motion compensation in head/brain CBCT and assessed in simulation and cadaver studies in comparison to a conventional gradient entropy metric.

*Results*. The 2-ResBlock architecture better reflected motion severity and extrapolated to unseen data, whereas 3-ResBlock was found more susceptible to overfitting, limiting its generalizability to unseen scenarios. DL-VIF outperformed gradient entropy in simulation studies yielding average multi-resolution structural similarity index (SSIM) improvement over uncompensated image of 0.068 and 0.034, respectively, referenced to motion-free images. DL-VIF was also more robust in motion compensation, evidenced by reduced variance in SSIM for various motion patterns (*σ*DL-VIF = 0.008 versus *σ*gradient entropy = 0.019). Similarly, in cadaver studies, DL-VIF demonstrated superior motion compensation compared to gradient entropy (an average SSIM improvement of 0.043 (5%) versus little improvement and even degradation in SSIM, respectively) and visually improved image quality even in severely motion-corrupted images.

*Conclusion.* The studies demonstrated the feasibility of building reference-free similarity metrics for quantification of motion-induced image quality degradation and distortion of anatomical structures in CBCT. DL-VIF provides a reliable surrogate for motion severity, penalizes unrealistic distortions, and presents a valuable new objective function for autofocus motion compensation in CBCT.

<p align='center'>
  <img src="/researchPosts/DLVIF/images/cadaverHeadMoCo.png" alt="Rigid Motion Compensation on Cadaver Head" title="LDSDE Results" style="zoom:100%;">
</p>



## [2021] Reference-Free, Learning-Based Rigid Motion Compensation in Cone-Beam CT for Interventional Neuroradiology

[H. Huang, J.H. Siewerdsen, W. Zbijewski, C. R. Weiss, T. Ehtiati, A. Sisniega, “Reference-Free, Learning-Based Rigid Motion Compensation in Cone-Beam CT for Interventional Neuroradiology, ” in AAPM 63rd Annual Meeting & Exhibition.](https://w4.aapm.org/meetings/2021AM/programInfo/programAbs.php?sid=9204& aid=58688)

Involuntary patient motion is the major impediment to CBCT guidance in interventional neuroradiology, diminishing image quality and causing repeat scans and increased dose. A robust autofocus method can mitigate this issue and it has been shown to reduce motion artifacts in head CBCT [1]. Autofocus methods use metrics that enforce properties associated to motion-free images, such as sharpness or sparsity of image gradients. Conventional metrics, however, do not guarantee the preservation of anatomical structures and can enforce images depicting unrealistic anatomy but featuring metric-friendly features, such as streak artifacts. Image similarity metrics, such as VIF, combine estimation of image quality and similarity to a reference, penalizing structures not compatible with the reference anatomy. However, matched reference images are seldom available in clinical scenarios. This work proposes DL-VIF, a deep convolutional neural network (CNN) capable of estimating VIF using only the motion contaminated image, without the need of a matched reference. DL-VIF serves as an anatomically realistic autofocus metric that opens the way to robust motion compensation with explicit penalization of unrealistic, yet sharp images.

<p align='center'>
  <img src="/researchPosts/DLVIF/images/AAPM.png" alt="Rigid Motion Compensation on Cadaver Head" title="LDSDE Results" style="zoom:100%;">
</p>
## [2021] Reference-Free, Learning-Based Image Similarity: Application to Motion Compensation in Cone-Beam CT

[H. Huang, J.H. Siewerdsen, W. Zbijewski, C. R. Weiss, T. Ehtiati, A. Sisniega, “Reference-Free, Learning-Based Image Similarity: Application to Motion Compensation in Cone-Beam CT,” in Proceedings of the 16th Virtual International Meeting on Fully 3D Image Reconstruction in Radiology and Nuclear Medicine](https://arxiv.org/abs/2110.04143) 

Cone-beam CT (CBCT) is increasingly used in the interventional suite, providing three-dimensional imaging for intervention planning, guidance during the procedure, or post-procedure assessment. However, the moderately long acquisition time of CBCT (~4 to 20 s) makes it susceptible to artifacts from patient motion during image acquisition. Recent work has shown promising reduction of artifacts caused by patient motion in CBCT with purely image-based autofocus motion compensation approaches. However, conventional autofocus metrics (e.g., gradient entropy) are agnostic to the realistic presentation of anatomical structures in the image and can yield unrealistic solutions. Image structural similarity metrics (e.g., visual information fidelity, VIF) combine measures of image quality and structural similarity to a reference image, offering a potentially ideal basis for autofocus metrics. However, matched motion-free reference images are usually not available. In this work, we propose a reference-free, learning-based image similarity metric obtained using a deep neural network (denoted DL-VIF). A convolutional neural network was trained on simulated motion-corrupted CBCT data to extract features associated with the structural components contributing to VIF. The DL-VIF showed correlation with conventional VIF in motion corrupted abdominal CBCT for both rigid (R 2 = 0.981 and slope = 0.987) and deformable (R 2 = 0.852 and slope = 0.928) motion. The DL-VIF was incorporated in an autofocus motion compensation framework, and its performance was compared against a conventional metric (gradient entropy). Motion compensation with DL-VIF resulted in more robust motion compensation (pointing to a lower susceptibility to local minima), and in improved performance (SSIM = 0.943 compared to 0.900 for gradient entropy). The development of autofocus metrics that recognize the integrity of anatomical structures in the image is an important step toward reliable motion compensation in scenarios of complex soft-tissue deformable motion in CBCT.

<p align='center'>
  <img src="/researchPosts/DLVIF/images/CTmeeting.png" alt="Rigid Motion Compensation on Cadaver Head" title="LDSDE Results" style="zoom:100%;">
</p>



